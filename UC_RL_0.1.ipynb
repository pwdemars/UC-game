{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Commitment by Reinforcement Learning\n",
    "\n",
    "A toy game in which a Reinforcement Learning (RL) agent tries to operate a (very very simple) power grid. The rules of the game are simple: At every stage, the agent is presented with the current on/off status of generators and the forecast demand for the next time period. Generators may be offline (represented by 0), online (represented by 2) or ready (represented by 1). Whenever an online generator is turned off, it is unavailable for one turn and is available to be turned on the following turn. Each generator has a fixed output. The agent's goal is to turn on generators to most closely match the forecast demand. \n",
    "\n",
    "Our agent is going to attempt to learn this model by Q-Learning, a popular RL technique. Much of this code has been adapted from the [excellent tutorial from Hvass Laboratories](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/5e5266b13d7b1bdfdef19a4bf8d8a0458817bdf4/16_Reinforcement_Learning.ipynb) on RL for Atari gameplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial settings for the game. \n",
    "\n",
    "num_gen = 5 #number of generators in the power system.\n",
    "num_actions = 2**num_gen #the full set of actions available to the agent; on or off for each generator.\n",
    "mean_demand = 25 #the mean demand for each time period.\n",
    "standard_deviation = 4 #the standard deviation for the normally distributed demand.\n",
    "\n",
    "#Generator settings.\n",
    "\n",
    "all_gens = np.array([1,3,6,10,15,20,22,40]) #the outputs for 8 generators which may be used in the game.\n",
    "gens = all_gens[:num_gen] #chooses the first num_gen of the generators for use in the game. \n",
    "\n",
    "#Define bounds on epsilon \n",
    "\n",
    "min_epsilon = 0.3\n",
    "max_epsilon = 0.85\n",
    "\n",
    "#Replay memory size - COME BACK TO THIS\n",
    "\n",
    "rm_size = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Dynamics\n",
    "\n",
    "The functions below define that game dynamics and are used for playing the game. The `Action` function takes an action and outputs a generation. `UpdateState` takes a state and action and outputs the new state. `Demand` generates a demand forecast from a normal distribution given by `mean_demand` and `standard_deviation` which are defined above. `Reward` calculates the absolute difference between the generation and demand. This is the agent's feedback as to the quality of its action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Action(action):\n",
    "    action = np.array(action)\n",
    "    generation = np.dot(gens, action)\n",
    "    return generation\n",
    "\n",
    "def UpdateState(state_original, action):\n",
    "    state_original = np.array(state_original)\n",
    "    action = np.array(action)\n",
    "    state_new = np.copy(state_original)\n",
    "    state_new[np.where(action == 0)] = 0\n",
    "    state_new[np.where(state_original == 0)] = 1\n",
    "    state_new[np.where(action == 1)] = 2\n",
    "    state_new[np.where(action == 1)]\n",
    "    state_new[np.argwhere((state_original == 1) & (action == 0)).flatten()] = 1\n",
    "\n",
    "    return np.array(state_new)\n",
    "\n",
    "def Demand(mean_demand): \n",
    "    demand = int(np.random.normal(mean_demand, standard_deviation))\n",
    "    return demand\n",
    "\n",
    "def Reward(demand, generation):\n",
    "    reward = -abs(demand - generation)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Actions\n",
    "\n",
    "The AvailableActions function returns the actions available to the agent based on the current status of generators. Remember, if a generator has been turned off last turn, it is not available to be committed until the next turn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcomb = np.asarray(list(itertools.product([0, 1], repeat=num_gen)))\n",
    "\n",
    "def AvailableActions(status):\n",
    "    all_available = np.copy(allcomb)\n",
    "    i = 0\n",
    "    for gen in status:\n",
    "        if gen == 0:\n",
    "            all_available = all_available[(all_available[:,i] == 0)]\n",
    "        i += 1            \n",
    "    return(all_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "In RL, we are ultimately looking for the optimal policy $\\pi^{*}$ mapping states to actions. One way to do this is to find the optimal action-value function $Q^{*}$. An action-value function gives the expected return from taking action $a$ in state $s$ and following $\\pi$ thereafter:\n",
    "\n",
    "$$Q^\\pi(s,a) = E_\\pi\\{R_t|s_t = s, a_t = a\\}$$\n",
    "\n",
    "If we find $Q^{*}$, the optimal poicy simply chooses the action with the highest Q-value at each time step.\n",
    "\n",
    "## Epsilon-Greedy Policy \n",
    "\n",
    "Without pre-encoding the transition dynamics (i.e. $\\Pr(s_{t+1} = s' | s_t = s, a_t = a)$), the agent must learn Q-values by trial and error. The agent begins by playing the game with an arbitrary, epislon-greedy policy to gain experience and begin estimating Q-values. The epsilon-greedy element of the agent's policy ensures that it explores different actions: it is forced to take a random action with probability $1-\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EpsilonGreedy(all_available, q_values, epsilon): \n",
    "    if np.random.random() < epsilon:\n",
    "        idx = np.random.randint(len(all_available))\n",
    "        action = all_available[idx]\n",
    "    else: \n",
    "        idx = np.argmax(q_values)\n",
    "        action = all_available(idx)\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "As the agent plays, it builds a Replay Memory. The Replay Memory stores the agent's experience in the form of $\\langle$state, action, reward, Q-value$\\rangle$ tuples. The Replay Memory is of a fixed size. Once it is full, we perform a backsweep and make the following update to the Q-values: \n",
    "\n",
    "$$Q(S_t, A_t)\\gets Q(S_t, A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "Here we are slowly (learning rate determined by $\\alpha$) updating our Q-values towards the Q-values we got from experience given by $R_{t+1}+\\gamma\\max_a Q(S_{t+1}, a)$ (called the TD-target). Crucially, the TD-target is calculated by bootstrapping: we measure the reward received from taking an action and add to our estimate for the Q-value of the state we arrived at. As we perform a backsweep, we use this bootstrapping to update the Q-values from the end of the Replay Memory (the last tuple we recorded) right the way back to the beginning. \n",
    "\n",
    "In addition, we multiply the bootstrapped Q-value estimate for state $S_{t+1}$ by a discount factor $\\gamma$. The discount factor is a way of addressing the so-called Credit Assignment Problem. That is we want to maintain a balance between attributing rewards to actions that came immediately before it, whilst recognising that earlier actions may have had an impact too. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory: \n",
    "    \n",
    "    def __init__(self, size, num_actions, discount_factor = 0.8):\n",
    "        \n",
    "        self.size = size\n",
    "        self.states = np.zeros(shape = [size] + [num_gen + 1], dtype = np.uint8)\n",
    "        self.q_values = np.zeros(shape = [size, num_actions], dtype = np.float)\n",
    "        self.q_values_old = np.zeros(shape = [size, num_actions], dtype = np.float)\n",
    "        self.actions = np.zeros(shape = size, dtype = np.int)\n",
    "        self.rewards = np.zeros(shape = size, dtype = np.int)\n",
    "        self.estimation_errors = np.zeros(shape = size, dtype = np.int)\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_used = 0\n",
    "        self.error_threshold = 0.1\n",
    "        \n",
    "    def is_full(self):\n",
    "        \n",
    "        return self.num_used == self.size\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.num_used = 0\n",
    "        \n",
    "    def add(self, state, q_values, action, reward):\n",
    "        \n",
    "        if not self.is_full():\n",
    "            \n",
    "            k = self.num_used\n",
    "            \n",
    "            self.num_used += 1\n",
    "\n",
    "            self.states[k] = state\n",
    "            self.q_values[k] = q_values\n",
    "            self.actions[k] = action\n",
    "            self.rewards[k] = reward\n",
    "            \n",
    "    def update_all_q_values(self):\n",
    "        \n",
    "        self.q_values_old[:] = self.q_values[:]\n",
    "        for k in reversed(range(self.num_used - 1)):\n",
    "            action = self.actions[k]\n",
    "            reward = self.rewards[k]\n",
    "            action_value = reward + self.discount_factor * np.max(self.q_values[k+1])\n",
    "        \n",
    "            self.estimation_errors[k] = abs(action_value - self.q_values[k,action])\n",
    "            \n",
    "            self.q_values[k,action] = action_value\n",
    "            \n",
    "    def random_batch(self, batch_size):\n",
    "        \n",
    "        idx = np.random.choice(self.size, batch_size, replace = False)\n",
    "        states_batch = self.states[idx]\n",
    "        q_values_batch = self.q_values[idx]\n",
    "        \n",
    "        return states_batch, q_values_batch\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "With the experience stored in the Replay Memory and the updated Q-values, we have everything we need to approximate the Q-value function using a neural network. The neural network fits a function that maps states to Q-values for all actions. \n",
    "\n",
    "in training, we take a random batch of states (input) and Q-values (output) and perform stochastic gradient descent to update the neural network weights. Our loss function is just the sum of squared errors between Q-value estimates produced by our current neural network and the 'actual' (remember these are still estimates, just better ones) Q-values from the Replay Memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_actions, replay_memory):\n",
    "        \n",
    "        self.replay_memory = replay_memory\n",
    "        \n",
    "        self.x = tf.placeholder(dtype = tf.float32, shape = (None, num_gen + 1))\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(dtype = tf.float32, shape = [])\n",
    "        \n",
    "        self.q_values_new = tf.placeholder(tf.float32, \n",
    "                                           shape = [None, num_actions],\n",
    "                                           name = 'q_values_new')\n",
    "        \n",
    "        init = tf.truncated_normal_initializer(mean=0.0, stddev=2e-2)\n",
    "        activation = tf.nn.relu\n",
    "        num_output = num_actions\n",
    "        \n",
    "        net = self.x\n",
    "        \n",
    "        net1 = tf.layers.dense(inputs=net, name='layer_fc1', units=6,\n",
    "                                          kernel_initializer=init, activation=activation, \n",
    "                                          reuse = tf.AUTO_REUSE)\n",
    "        \n",
    "        net2 = tf.layers.dense(inputs=net1, name='layer_fc_out', units=num_output,\n",
    "                                          kernel_initializer=init, activation=None, \n",
    "                                          reuse = tf.AUTO_REUSE)\n",
    "        \n",
    "\n",
    "        self.q_values = net2\n",
    "        \n",
    "        squared_error = tf.square(self.q_values - self.q_values_new)\n",
    "        sum_squared_error = tf.reduce_sum(squared_error, axis=1)\n",
    "        self.loss = tf.reduce_mean(sum_squared_error)\n",
    "\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)    \n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def get_q_values(self, states):\n",
    "        \n",
    "        feed_dict = {self.x: states}\n",
    "        values = self.session.run(self.q_values, feed_dict = feed_dict)\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    \n",
    "    \n",
    "    def optimize(self, min_epochs = 1.0, max_epochs = 10, batch_size = 10,\n",
    "                 loss_limit = 0.01, learning_rate = 0.01):\n",
    "        \n",
    "        loss_history = np.zeros(100, dtype=float)\n",
    "        \n",
    "        iterations_per_epoch = self.replay_memory.num_used / batch_size\n",
    "        min_iterations = int(iterations_per_epoch * min_epochs)\n",
    "        max_iterations = int(iterations_per_epoch * max_epochs)\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            \n",
    "            state_batch, q_values_batch = self.replay_memory.random_batch(batch_size)\n",
    "            \n",
    "            feed_dict = {self.x: state_batch,\n",
    "                         self.q_values_new: q_values_batch,\n",
    "                         self.learning_rate: learning_rate}\n",
    "            \n",
    "            loss_val, _ = self.session.run([self.loss, self.optimizer], \n",
    "                                           feed_dict = feed_dict)\n",
    "            \n",
    "            loss_history = np.roll(loss_history, 1)\n",
    "            loss_history[0] = loss_val\n",
    "            \n",
    "            loss_mean = np.mean(loss_history)\n",
    "            \n",
    "            if i > min_iterations and loss_mean < loss_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation \n",
    "\n",
    "Now we are ready to simulate and begin training the RL agent. At each iteration, the agent will act following an epsilon-greedy policy based on Q-values estimated by the neural network. Every $\\langle$state, action, reward, Q-value$\\rangle$ is recorded in the Replay Memory which, when full, is sampled from to train the neural network using stochastic gradient descent. \n",
    "\n",
    "The second part of the `simulate` function measures the agents performance by playing with a greedier policy and recording the average reward. We also record the rewards of a benchmark random policy for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Simulate(num_iterations):\n",
    "    \n",
    "    gen_status = np.random.randint(3, size = num_gen)\n",
    "\n",
    "    demand = Demand(mean_demand)\n",
    "    \n",
    "    state = np.append(gen_status, demand).reshape(1, num_gen + 1)\n",
    "        \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "    #Defining the control signal for epsilon. Epsilon increments by 1/num_iterations\n",
    "    #at each iteration. Starts at min_epsilon, goes to max_epsilon\n",
    "        \n",
    "    #   epsilon = min_epsilon + j*((max_epsilon - min_epsilon)/num_iterations)\n",
    "        epsilon = 0.8\n",
    "              \n",
    "        for i in range(rm_size):\n",
    "            \n",
    "            qs = nn.get_q_values(state)\n",
    "            \n",
    "            available_actions = AvailableActions(state[0][:num_gen])\n",
    "            \n",
    "            available_actions_idx = list()\n",
    "            \n",
    "            for act in available_actions:\n",
    "                available_actions_idx.append(int(np.where(np.all(allcomb == act, axis = 1))[0]))\n",
    "            \n",
    "            if np.random.rand() < epsilon:\n",
    "                \n",
    "                action = np.amax(qs[0][available_actions_idx])\n",
    "                action = int(np.where(qs[0] == action)[0])\n",
    "                generation = Action(allcomb[action])    \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                action = np.random.choice(available_actions_idx)\n",
    "                generation = Action(allcomb[action]) \n",
    "            \n",
    "            reward = Reward(demand, generation)\n",
    "            rm.add(state, qs, action, reward)\n",
    "        \n",
    "            state = UpdateState(state[0][0:num_gen], allcomb[action])\n",
    "            demand = Demand(mean_demand)\n",
    "            state = np.append(state, demand).reshape(1,num_gen + 1)\n",
    "            \n",
    "        rm.update_all_q_values()\n",
    "        \n",
    "        nn.optimize()\n",
    "        \n",
    "        rm.reset()\n",
    "    \n",
    "        totalreward = 0\n",
    "        total_random_reward = 0\n",
    "        \n",
    "        #Measuring the performance of the RL agent.\n",
    "        \n",
    "        if j % 5 == 0:\n",
    "        \n",
    "            for k in range(200):\n",
    "    \n",
    "                \n",
    "                qs = nn.get_q_values(state)\n",
    "                available_actions = AvailableActions(state[0][:num_gen])\n",
    "                available_actions_idx = list()\n",
    "                \n",
    "                for act in available_actions:\n",
    "                    available_actions_idx.append(int(np.where(np.all(allcomb == act, axis = 1))[0]))\n",
    "                    \n",
    "                if np.random.rand() < 0.99:\n",
    "                    \n",
    "                    action = np.amax(qs[0][available_actions_idx])\n",
    "                    action = int(np.where(qs[0] == action)[0])\n",
    "                    generation = Action(allcomb[action])\n",
    "                    \n",
    "        \n",
    "                else:\n",
    "                    \n",
    "                    action = np.random.choice(available_actions_idx)\n",
    "                    generation = Action(allcomb[action])\n",
    "                \n",
    "                reward = Reward(demand, generation)  \n",
    "                totalreward = totalreward + reward\n",
    "                \n",
    "                state = UpdateState(state[0][0:num_gen], allcomb[action])\n",
    "                demand = Demand(mean_demand)\n",
    "                state = np.append(state, demand).reshape(1,num_gen + 1)\n",
    "            \n",
    "                #Try a random action for reference:\n",
    "                \n",
    "                random_generation = Action(allcomb[np.random.choice(available_actions_idx)])\n",
    "                random_reward = Reward(demand, random_generation)\n",
    "                total_random_reward = total_random_reward + random_reward\n",
    "            \n",
    "            rewards_data.append(float(totalreward)/200)\n",
    "            random_rewards_data.append(float(total_random_reward)/200)\n",
    "        \n",
    "    plt.plot(rewards_data)\n",
    "    plt.plot(random_rewards_data)\n",
    "    plt.legend([\"rewards\", \"random rewards\"], loc = \"upper left\")\n",
    "    plt.savefig('plt.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's intialise the Replay Memory and neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm = ReplayMemory(rm_size, num_actions)  \n",
    "nn = NeuralNetwork(num_actions, rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make empty lists for recording the rewards during testing, as well as the random rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards_data = list()\n",
    "random_rewards_data = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some simulations. 100 iterations to begin with. Remember, a single iteration fills up the Replay Memory, then trains the neural network from it. We have set the size of the Replay Memory with `rm_size`, so the total number of turns taken by the agent is `rm_size` * `num_iterations`. We can repeat this as many times as we like to improve our RL agent, and observe the improvements on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Simulate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0291d7f4bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Simulate' is not defined"
     ]
    }
   ],
   "source": [
    "Simulate(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
